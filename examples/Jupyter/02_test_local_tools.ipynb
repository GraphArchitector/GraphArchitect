{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тестирование локальных инструментов\n",
    "\n",
    "Локальные инструменты работают без внешних API:\n",
    "- Zero-Shot Classifier\n",
    "- Text Vectorizer  \n",
    "- Audio Recognizer\n",
    "- Video Describer\n",
    "- VQA (Visual Question Answering)\n",
    "\n",
    "Требуется: PyTorch, transformers, модели Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphArchitect добавлен в путь\n",
      "  Путь: C:\\Users\\ZZZ\\Documents\\GitHub\\Python\\GraphArchitect\\src\\GraphArchitectLib\n"
     ]
    }
   ],
   "source": [
    "# Настройка\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "grapharchitect_path = Path.cwd().parent.parent / \"src\" / \"GraphArchitectLib\"\n",
    "sys.path.insert(0, str(grapharchitect_path))\n",
    "\n",
    "print(\"GraphArchitect добавлен в путь\")\n",
    "print(f\"  Путь: {grapharchitect_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Zero-Shot Text Classifier\n",
    "\n",
    "Классификация текста без обучения, используя MNLI модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:faiss.loader:Loading faiss with AVX2 support.\n",
      "INFO:faiss.loader:Successfully loaded faiss with AVX2 support.\n",
      "C:\\Users\\ZZZ\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Invalid URL '/api/resolve-cache/models/facebook/bart-large-mnli/d7645e127eaf1aefc7862fd59a17a5aa8558b8ce/config.json?%2Ffacebook%2Fbart-large-mnli%2Fresolve%2Fmain%2Fconfig.json=&etag=%22dfc96c728b40cc131e46b1c424c8e072d52b6d74%22': No scheme supplied. Perhaps you meant https:///api/resolve-cache/models/facebook/bart-large-mnli/d7645e127eaf1aefc7862fd59a17a5aa8558b8ce/config.json?%2Ffacebook%2Fbart-large-mnli%2Fresolve%2Fmain%2Fconfig.json=&etag=%22dfc96c728b40cc131e46b1c424c8e072d52b6d74%22?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ошибка: Can't load config for 'facebook/bart-large-mnli'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'facebook/bart-large-mnli' is the correct path to a directory containing a config.json file\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from grapharchitect.tools.LocalTools.Classifiers.ZeroShotTextClassifier import ZeroShotClassifier\n",
    "    \n",
    "    # Создание классификатора\n",
    "    classifier = ZeroShotClassifier(\n",
    "        model_or_path=\"facebook/bart-large-mnli\"\n",
    "    )\n",
    "    \n",
    "    print(\"Zero-Shot Classifier создан\")\n",
    "    print(f\"  Модель: {classifier.model}\")\n",
    "    \n",
    "    # Тестовая классификация\n",
    "    text = \"Отличный продукт, очень доволен!\"\n",
    "    labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "    \n",
    "    print(f\"\\nТекст: {text}\")\n",
    "    print(f\"Категории: {labels}\")\n",
    "    \n",
    "    # Раскомментируйте для реального вызова:\n",
    "    result = classifier.classify(text, labels)\n",
    "    print(f\"\\nРезультат: {result}\")\n",
    "    \n",
    "    print(\"\\nТребуется PyTorch и transformers\")\n",
    "    print(\"  pip install torch transformers\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Vectorizer\n",
    "\n",
    "Векторизация текста используя E5 модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/intfloat/e5-base-v2/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/intfloat/e5-base-v2/f52bf8ec8c7124536f0efb74aca902b2995e5bcd/config.json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://huggingface.co/api/resolve-cache/models/intfloat/e5-base-v2/f52bf8ec8c7124536f0efb74aca902b2995e5bcd/config.json \"HTTP/1.1 200 OK\"\n",
      "c:\\Users\\any\\source\\python\\GraphArchitect\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\any\\.cache\\huggingface\\hub\\models--intfloat--e5-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/intfloat/e5-base-v2/resolve/main/model.safetensors \"HTTP/1.1 302 Found\"\n",
      "INFO:httpx:HTTP Request: GET https://huggingface.co/api/models/intfloat/e5-base-v2/xet-read-token/f52bf8ec8c7124536f0efb74aca902b2995e5bcd \"HTTP/1.1 200 OK\"\n",
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 429.21it/s, Materializing param=pooler.dense.weight]                               \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: intfloat/e5-base-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/intfloat/e5-base-v2/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/intfloat/e5-base-v2/f52bf8ec8c7124536f0efb74aca902b2995e5bcd/config.json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/intfloat/e5-base-v2/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/intfloat/e5-base-v2/f52bf8ec8c7124536f0efb74aca902b2995e5bcd/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://huggingface.co/api/resolve-cache/models/intfloat/e5-base-v2/f52bf8ec8c7124536f0efb74aca902b2995e5bcd/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://huggingface.co/api/models/intfloat/e5-base-v2/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
      "INFO:httpx:HTTP Request: GET https://huggingface.co/api/models/intfloat/e5-base-v2/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/intfloat/e5-base-v2/resolve/main/vocab.txt \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/intfloat/e5-base-v2/f52bf8ec8c7124536f0efb74aca902b2995e5bcd/vocab.txt \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://huggingface.co/api/resolve-cache/models/intfloat/e5-base-v2/f52bf8ec8c7124536f0efb74aca902b2995e5bcd/vocab.txt \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/intfloat/e5-base-v2/resolve/main/tokenizer.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/intfloat/e5-base-v2/f52bf8ec8c7124536f0efb74aca902b2995e5bcd/tokenizer.json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://huggingface.co/api/resolve-cache/models/intfloat/e5-base-v2/f52bf8ec8c7124536f0efb74aca902b2995e5bcd/tokenizer.json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/intfloat/e5-base-v2/resolve/main/added_tokens.json \"HTTP/1.1 404 Not Found\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/intfloat/e5-base-v2/resolve/main/special_tokens_map.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/intfloat/e5-base-v2/f52bf8ec8c7124536f0efb74aca902b2995e5bcd/special_tokens_map.json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://huggingface.co/api/resolve-cache/models/intfloat/e5-base-v2/f52bf8ec8c7124536f0efb74aca902b2995e5bcd/special_tokens_map.json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/intfloat/e5-base-v2/resolve/main/chat_template.jinja \"HTTP/1.1 404 Not Found\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Text Vectorizer создан\n",
      "  Модель: BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n",
      "\n",
      "Тексты для векторизации: 3\n",
      "Эмбеддинги: (3, 768)\n",
      "Сходство 'ML' и 'AI': 0.821\n",
      "\n",
      "⚠ Требуется PyTorch и transformers\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from grapharchitect.tools.LocalTools.TextVectorize import TextVectorizer\n",
    "    \n",
    "    # Создание векторайзера\n",
    "    vectorizer = TextVectorizer(\n",
    "        model_or_path=\"intfloat/e5-base-v2\"\n",
    "    )\n",
    "    \n",
    "    print(\"Text Vectorizer создан\")\n",
    "    print(f\"  Модель: {vectorizer.model}\")\n",
    "    \n",
    "    # Тестовая векторизация\n",
    "    texts = [\n",
    "        \"Машинное обучение\",\n",
    "        \"Искусственный интеллект\",\n",
    "        \"Погода сегодня\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nТексты для векторизации: {len(texts)}\")\n",
    "    \n",
    "    # Раскомментируйте для реального вызова:\n",
    "    embeddings = vectorizer.vectorize(texts)\n",
    "    print(f\"Эмбеддинги: {embeddings.shape}\")\n",
    "    \n",
    "    # Косинусное сходство первых двух\n",
    "    from numpy import dot\n",
    "    from numpy.linalg import norm\n",
    "    similarity = dot(embeddings[0], embeddings[1]) / (norm(embeddings[0]) * norm(embeddings[1]))\n",
    "    print(f\"Сходство 'ML' и 'AI': {similarity:.3f}\")\n",
    "    \n",
    "    print(\"\\nТребуется PyTorch и transformers\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. VQA (Visual Question Answering)\n",
    "\n",
    "Ответы на вопросы по изображениям используя BLIP модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Salesforce/blip-vqa-base/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Salesforce/blip-vqa-base/787b3d35d57e49572baabd22884b3d5a05acf072/config.json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Salesforce/blip-vqa-base/resolve/main/model.safetensors \"HTTP/1.1 302 Found\"\n",
      "Loading weights: 100%|██████████| 472/472 [00:01<00:00, 429.12it/s, Materializing param=vision_model.post_layernorm.weight]                                       \n",
      "The tied weights mapping and config for this model specifies to tie text_decoder.bert.embeddings.word_embeddings.weight to text_decoder.cls.predictions.decoder.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "\u001b[1mBlipForConditionalGeneration LOAD REPORT\u001b[0m from: Salesforce/blip-vqa-base\n",
      "Key                                                                        | Status     |  | \n",
      "---------------------------------------------------------------------------+------------+--+-\n",
      "text_encoder.encoder.layer.{0...11}.attention.self.key.bias                | UNEXPECTED |  | \n",
      "text_encoder.encoder.layer.{0...11}.crossattention.output.dense.weight     | UNEXPECTED |  | \n",
      "text_encoder.encoder.layer.{0...11}.crossattention.output.LayerNorm.bias   | UNEXPECTED |  | \n",
      "text_encoder.encoder.layer.{0...11}.intermediate.dense.weight              | UNEXPECTED |  | \n",
      "text_encoder.encoder.layer.{0...11}.crossattention.self.query.bias         | UNEXPECTED |  | \n",
      "text_encoder.encoder.layer.{0...11}.crossattention.self.query.weight       | UNEXPECTED |  | \n",
      "text_encoder.encoder.layer.{0...11}.output.LayerNorm.bias                  | UNEXPECTED |  | \n",
      "text_encoder.encoder.layer.{0...11}.crossattention.self.key.bias           | UNEXPECTED |  | \n",
      "text_encoder.encoder.layer.{0...11}.attention.output.LayerNorm.weight      | UNEXPECTED |  | \n",
      "text_encoder.encoder.layer.{0...11}.intermediate.dense.bias                | UNEXPECTED |  | \n",
      "text_encoder.encoder.layer.{0...11}.output.dense.bias                      | UNEXPECTED |  | \n",
      "text_encoder.encoder.layer.{0...11}.crossattention.output.LayerNorm.weight | UNEXPECTED |  | \n",
      "text_encoder.encoder.layer.{0...11}.attention.self.value.weight            | UNEXPECTED |  | \n",
      "text_encoder.encoder.layer.{0...11}.output.dense.weight                    | UNEXPECTED |  | \n",
      "text_encoder.embeddings.word_embeddings.weight                             | UNEXPECTED |  | \n",
      "text_encoder.encoder.layer.{0...11}.crossattention.self.key.weight         | UNEXPECTED |  | \n",
      "text_encoder.encoder.layer.{0...11}.attention.output.dense.weight          | UNEXPECTED |  | \n",
      "text_encoder.encoder.layer.{0...11}.attention.self.key.weight              | UNEXPECTED |  | \n",
      "text_encoder.encoder.layer.{0...11}.crossattention.self.value.weight       | UNEXPECTED |  | \n",
      "text_encoder.encoder.layer.{0...11}.attention.self.query.weight            | UNEXPECTED |  | \n",
      "text_encoder.embeddings.LayerNorm.bias                                     | UNEXPECTED |  | \n",
      "text_encoder.encoder.layer.{0...11}.crossattention.self.value.bias         | UNEXPECTED |  | \n",
      "text_encoder.embeddings.position_embeddings.weight                         | UNEXPECTED |  | \n",
      "text_encoder.encoder.layer.{0...11}.crossattention.output.dense.bias       | UNEXPECTED |  | \n",
      "text_encoder.encoder.layer.{0...11}.output.LayerNorm.weight                | UNEXPECTED |  | \n",
      "text_encoder.encoder.layer.{0...11}.attention.self.value.bias              | UNEXPECTED |  | \n",
      "text_encoder.encoder.layer.{0...11}.attention.output.LayerNorm.bias        | UNEXPECTED |  | \n",
      "text_encoder.encoder.layer.{0...11}.attention.output.dense.bias            | UNEXPECTED |  | \n",
      "text_encoder.encoder.layer.{0...11}.attention.self.query.bias              | UNEXPECTED |  | \n",
      "text_encoder.embeddings.LayerNorm.weight                                   | UNEXPECTED |  | \n",
      "text_decoder.bert.embeddings.position_ids                                  | UNEXPECTED |  | \n",
      "text_encoder.embeddings.position_ids                                       | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Salesforce/blip-vqa-base/resolve/main/generation_config.json \"HTTP/1.1 404 Not Found\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Salesforce/blip-vqa-base/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Salesforce/blip-vqa-base/787b3d35d57e49572baabd22884b3d5a05acf072/config.json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://huggingface.co/api/models/Salesforce/blip-vqa-base/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Salesforce/blip-vqa-base/resolve/main/processor_config.json \"HTTP/1.1 404 Not Found\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Salesforce/blip-vqa-base/resolve/main/chat_template.json \"HTTP/1.1 404 Not Found\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Salesforce/blip-vqa-base/resolve/main/chat_template.jinja \"HTTP/1.1 404 Not Found\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Salesforce/blip-vqa-base/resolve/main/audio_tokenizer_config.json \"HTTP/1.1 404 Not Found\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Salesforce/blip-vqa-base/resolve/main/processor_config.json \"HTTP/1.1 404 Not Found\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Salesforce/blip-vqa-base/resolve/main/preprocessor_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Salesforce/blip-vqa-base/787b3d35d57e49572baabd22884b3d5a05acf072/preprocessor_config.json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Salesforce/blip-vqa-base/resolve/main/processor_config.json \"HTTP/1.1 404 Not Found\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Salesforce/blip-vqa-base/resolve/main/preprocessor_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Salesforce/blip-vqa-base/787b3d35d57e49572baabd22884b3d5a05acf072/preprocessor_config.json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Salesforce/blip-vqa-base/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Salesforce/blip-vqa-base/787b3d35d57e49572baabd22884b3d5a05acf072/config.json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Salesforce/blip-vqa-base/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Salesforce/blip-vqa-base/787b3d35d57e49572baabd22884b3d5a05acf072/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://huggingface.co/api/models/Salesforce/blip-vqa-base/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
      "INFO:httpx:HTTP Request: GET https://huggingface.co/api/models/Salesforce/blip-vqa-base/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ VQA модель создана\n",
      "  Модель: BlipForConditionalGeneration(\n",
      "  (vision_model): BlipVisionModel(\n",
      "    (embeddings): BlipVisionEmbeddings(\n",
      "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "    )\n",
      "    (encoder): BlipEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x BlipEncoderLayer(\n",
      "          (self_attn): BlipAttention(\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (projection): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): BlipMLP(\n",
      "            (activation_fn): GELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (text_decoder): BlipTextLMHeadModel(\n",
      "    (bert): BlipTextModel(\n",
      "      (embeddings): BlipTextEmbeddings(\n",
      "        (word_embeddings): Embedding(30524, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (encoder): BlipTextEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-11): 12 x BlipTextLayer(\n",
      "            (attention): BlipTextAttention(\n",
      "              (self): BlipTextSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (output): BlipTextSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (crossattention): BlipTextAttention(\n",
      "              (self): BlipTextSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (output): BlipTextSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BlipTextIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BlipTextOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (cls): BlipTextOnlyMLMHead(\n",
      "      (predictions): BlipTextLMPredictionHead(\n",
      "        (transform): BlipTextPredictionHeadTransform(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (transform_act_fn): GELUActivation()\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (decoder): Linear(in_features=768, out_features=30524, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "⚠ Для теста подготовьте изображение:\n",
      "  from PIL import Image\n",
      "  image = Image.open('test.jpg')\n",
      "  answer = vqa.answer(image, 'What is in the image?')\n",
      "\n",
      "⚠ Требуется: torch, transformers, PIL\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from grapharchitect.tools.LocalTools.VQA import GenerativeVQA\n",
    "    \n",
    "    # Создание VQA модели\n",
    "    vqa = GenerativeVQA(\n",
    "        model_or_path=\"Salesforce/blip-vqa-base\"\n",
    "    )\n",
    "    \n",
    "    print(\"VQA модель создана\")\n",
    "    print(f\"  Модель: {vqa.model}\")\n",
    "    \n",
    "    # Для теста нужно изображение\n",
    "    print(\"\\n⚠ Для теста подготовьте изображение:\")\n",
    "    print(\"  from PIL import Image\")\n",
    "    print(\"  image = Image.open('test.jpg')\")\n",
    "    print(\"  answer = vqa.answer(image, 'What is in the image?')\")\n",
    "\n",
    "    from PIL import Image\n",
    "    image = Image.open('test.jpg')\n",
    "    answer = vqa.answer(image, 'What is in the image?')\n",
    "    \n",
    "    print(\"\\nТребуется: torch, transformers, PIL\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Тестирование в GraphArchitect\n",
    "\n",
    "Использование локальных инструментов в полном workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphArchitect orchestrator инициализирован\n",
      "\n",
      "Можно использовать любые локальные инструменты в workflow\n",
      "Они будут автоматически интегрированы в граф и выбраны через softmax\n"
     ]
    }
   ],
   "source": [
    "from grapharchitect.services.execution.execution_orchestrator import ExecutionOrchestrator\n",
    "from grapharchitect.services.selection.instrument_selector import InstrumentSelector\n",
    "from grapharchitect.services.graph_strategy_finder import GraphStrategyFinder\n",
    "from grapharchitect.services.embedding.simple_embedding_service import SimpleEmbeddingService\n",
    "from grapharchitect.entities.task_definition import TaskDefinition\n",
    "from grapharchitect.entities.connectors.connector import Connector\n",
    "\n",
    "# Инициализация\n",
    "embedding = SimpleEmbeddingService(dimension=384)\n",
    "selector = InstrumentSelector(temperature_constant=1.0)\n",
    "finder = GraphStrategyFinder()\n",
    "orchestrator = ExecutionOrchestrator(embedding, selector, finder)\n",
    "\n",
    "print(\"GraphArchitect orchestrator инициализирован\")\n",
    "print(\"\\nМожно использовать любые локальные инструменты в workflow\")\n",
    "print(\"Они будут автоматически интегрированы в граф и выбраны через softmax\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Итоги\n",
    "\n",
    "**Локальные инструменты**:\n",
    "- Работают офлайн (без API)\n",
    "- Бесплатные (только GPU/CPU)\n",
    "- Privacy (данные не уходят)\n",
    "\n",
    "**Требования**:\n",
    "- PyTorch\n",
    "- Transformers\n",
    "- Модели Hugging Face\n",
    "- GPU рекомендуется\n",
    "\n",
    "**Применение**:\n",
    "- Когда нужна приватность\n",
    "- Когда нет интернета\n",
    "- Когда нужно сэкономить на API\n",
    "\n",
    "**См. также**: `03_integration_example.ipynb` для полных примеров интеграции"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
