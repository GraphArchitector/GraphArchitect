{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тестирование RLAIF (LLM как судья)\n",
    "\n",
    "Reinforcement Learning from AI Feedback:\n",
    "- LLM оценивает качество ответов\n",
    "- Автоматическое обучение на основе оценок\n",
    "- Измерение улучшения системы\n",
    "\n",
    "Требуется: OPENROUTER_API_KEY или VLLM сервер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ OPENROUTER_API_KEY установлен - будут реальные оценки\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "grapharchitect_path = Path.cwd().parent.parent / \"src\" / \"GraphArchitectLib\"\n",
    "sys.path.insert(0, str(grapharchitect_path))\n",
    "\n",
    "# Проверка API ключа\n",
    "HAS_API_KEY = bool(os.getenv('OPENROUTER_API_KEY'))\n",
    "\n",
    "if HAS_API_KEY:\n",
    "    print(\"✓ OPENROUTER_API_KEY установлен - будут реальные оценки\")\n",
    "else:\n",
    "    print(\"⚠ OPENROUTER_API_KEY не установлен - симуляция\")\n",
    "    print(\"  Для реальных оценок: set OPENROUTER_API_KEY=your-key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Создание LLM Критика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:faiss.loader:Loading faiss with AVX2 support.\n",
      "INFO:faiss.loader:Successfully loaded faiss with AVX2 support.\n",
      "INFO:grapharchitect.tools.ApiTools.OpenRouterTool.openrouter_llm:OpenRouter инициализирован: модель openai/gpt-3.5-turbo\n",
      "INFO:grapharchitect.services.rlaif.llm_critic:OpenRouter backend initialized successfully\n",
      "INFO:grapharchitect.services.rlaif.llm_critic:LLM Critic initialized: backend=openrouter, model=openai/gpt-3.5-turbo, detailed=True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ LLM Критик создан\n",
      "  Модель: openai/gpt-3.5-turbo\n",
      "  Температура: 0.2\n"
     ]
    }
   ],
   "source": [
    "from grapharchitect.services.rlaif.llm_critic import LLMCritic\n",
    "\n",
    "if HAS_API_KEY:\n",
    "    critic = LLMCritic(\n",
    "        backend=\"openrouter\",\n",
    "        model_name=\"openai/gpt-3.5-turbo\",\n",
    "        temperature=0.2,  # Низкая для consistency\n",
    "        detailed_evaluation=True\n",
    "    )\n",
    "    \n",
    "    print(\"✓ LLM Критик создан\")\n",
    "    print(f\"  Модель: openai/gpt-3.5-turbo\")\n",
    "    print(f\"  Температура: 0.2\")\n",
    "else:\n",
    "    print(\"Симуляция LLM критика\")\n",
    "    critic = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Тест оценки хорошего ответа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Задача: Классифицировать отзыв по тональности\n",
      "\n",
      "Ответ:\n",
      "[Классификация]\n",
      "Отзыв: ПОЗИТИВНЫЙ\n",
      "\n",
      "Обоснование:\n",
      "- Фразы \"отличный продукт\", \"очень доволен\" указывают на позитив\n",
      "- Нет негативных формулировок\n",
      "- Общий тон восторженный\n",
      "\n",
      "Уверенность: 95%\n",
      "\n",
      "Оценка LLM критиком...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:grapharchitect.tools.ApiTools.OpenRouterTool.openrouter_llm:Использовано токенов: prompt=648, completion=114, total=762\n",
      "INFO:grapharchitect.services.rlaif.llm_critic:Evaluation completed: task_id=None, overall=0.900, correctness=1.000, completeness=0.900, relevance=1.000, clarity=0.900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты оценки:\n",
      "  Общий балл: 0.90/1.0\n",
      "  Правильность: 1.00\n",
      "  Полнота: 0.90\n",
      "  Релевантность: 1.00\n",
      "  Ясность: 0.90\n",
      "\n",
      "  Обоснование: The answer provides a correct and accurate classification of the review's sentiment, supported by relevant evidence. It covers all essential aspects of the task with a high level of relevance and clarity.\n"
     ]
    }
   ],
   "source": [
    "task = \"Классифицировать отзыв по тональности\"\n",
    "\n",
    "good_answer = \"\"\"[Классификация]\n",
    "Отзыв: ПОЗИТИВНЫЙ\n",
    "\n",
    "Обоснование:\n",
    "- Фразы \"отличный продукт\", \"очень доволен\" указывают на позитив\n",
    "- Нет негативных формулировок\n",
    "- Общий тон восторженный\n",
    "\n",
    "Уверенность: 95%\"\"\"\n",
    "\n",
    "print(f\"Задача: {task}\")\n",
    "print(f\"\\nОтвет:\\n{good_answer}\\n\")\n",
    "\n",
    "if HAS_API_KEY and critic:\n",
    "    print(\"Оценка LLM критиком...\\n\")\n",
    "    \n",
    "    score = critic.evaluate_answer(task, good_answer)\n",
    "    \n",
    "    print(\"Результаты оценки:\")\n",
    "    print(f\"  Общий балл: {score.overall_score:.2f}/1.0\")\n",
    "    print(f\"  Правильность: {score.correctness:.2f}\")\n",
    "    print(f\"  Полнота: {score.completeness:.2f}\")\n",
    "    print(f\"  Релевантность: {score.relevance:.2f}\")\n",
    "    print(f\"  Ясность: {score.clarity:.2f}\")\n",
    "    print(f\"\\n  Обоснование: {score.reasoning}\")\n",
    "else:\n",
    "    print(\"Симуляция оценки: ~0.90 (хороший ответ)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Тест оценки плохого ответа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Задача: Классифицировать отзыв по тональности\n",
      "\n",
      "Ответ: Позитивный.\n",
      "\n",
      "Оценка LLM критиком...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:grapharchitect.tools.ApiTools.OpenRouterTool.openrouter_llm:Использовано токенов: prompt=558, completion=117, total=675\n",
      "INFO:grapharchitect.services.rlaif.llm_critic:Evaluation completed: task_id=None, overall=0.900, correctness=1.000, completeness=0.800, relevance=1.000, clarity=1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты оценки:\n",
      "  Общий балл: 0.90/1.0\n",
      "  Правильность: 1.00\n",
      "  Полнота: 0.80 ⚠ (низкая)\n",
      "  Релевантность: 1.00\n",
      "  Ясность: 1.00 ⚠ (низкая)\n",
      "\n",
      "  Обоснование: The response is correct in identifying the sentiment as positive, relevant to the task, clear, and logically sound. It is comprehensive but could benefit from mentioning the methodology or features used for sentiment analysis.\n",
      "  Предложения: To improve completeness, consider mentioning the specific features or methods used for sentiment analysis to provide a more detailed and comprehensive response.\n"
     ]
    }
   ],
   "source": [
    "bad_answer = \"Позитивный.\"  # Слишком короткий, нет обоснования\n",
    "\n",
    "print(f\"Задача: {task}\")\n",
    "print(f\"\\nОтвет: {bad_answer}\\n\")\n",
    "\n",
    "if HAS_API_KEY and critic:\n",
    "    print(\"Оценка LLM критиком...\\n\")\n",
    "    \n",
    "    score = critic.evaluate_answer(task, bad_answer)\n",
    "    \n",
    "    print(\"Результаты оценки:\")\n",
    "    print(f\"  Общий балл: {score.overall_score:.2f}/1.0\")\n",
    "    print(f\"  Правильность: {score.correctness:.2f}\")\n",
    "    print(f\"  Полнота: {score.completeness:.2f} ⚠ (низкая)\")\n",
    "    print(f\"  Релевантность: {score.relevance:.2f}\")\n",
    "    print(f\"  Ясность: {score.clarity:.2f} ⚠ (низкая)\")\n",
    "    print(f\"\\n  Обоснование: {score.reasoning}\")\n",
    "    \n",
    "    if score.suggestions:\n",
    "        print(f\"  Предложения: {score.suggestions}\")\n",
    "else:\n",
    "    print(\"Симуляция оценки: ~0.45 (плохой ответ - неполный)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RLAIF Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем полный workflow с обучением\n",
    "from grapharchitect.services.rlaif.rlaif_trainer import RLAIFTrainer\n",
    "from grapharchitect.services.training.training_orchestrator import TrainingOrchestrator\n",
    "from grapharchitect.services.execution.execution_orchestrator import ExecutionOrchestrator\n",
    "from grapharchitect.services.selection.instrument_selector import InstrumentSelector\n",
    "from grapharchitect.services.graph_strategy_finder import GraphStrategyFinder\n",
    "from grapharchitect.entities.task_definition import TaskDefinition\n",
    "from grapharchitect.entities.connectors.connector import Connector\n",
    "\n",
    "# Инициализация\n",
    "training_orch = TrainingOrchestrator(learning_rate=0.02)\n",
    "selector = InstrumentSelector(temperature_constant=1.0)\n",
    "finder = GraphStrategyFinder()\n",
    "orchestrator = ExecutionOrchestrator(embedding, selector, finder)\n",
    "\n",
    "if HAS_API_KEY and critic:\n",
    "    rlaif_trainer = RLAIFTrainer(\n",
    "        llm_critic=critic,\n",
    "        training_orchestrator=training_orch\n",
    "    )\n",
    "    \n",
    "    print(\"✓ RLAIF Trainer инициализирован\")\n",
    "else:\n",
    "    print(\"Симуляция RLAIF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выполнение и оценка\n",
    "task_def = TaskDefinition(\n",
    "    description=\"Классифицировать отзыв\",\n",
    "    input_connector=Connector(\"text\", \"question\"),\n",
    "    output_connector=Connector(\"text\", \"answer\"),\n",
    "    input_data=\"Отличный продукт!\"\n",
    ")\n",
    "\n",
    "# Выполнение\n",
    "context = orchestrator.execute_task(task_def, tools[:1], path_limit=1, top_k=1)\n",
    "\n",
    "print(f\"Выполнено: {context.status.value}\")\n",
    "print(f\"Результат: {context.result}\")\n",
    "\n",
    "# RLAIF оценка и обучение\n",
    "if HAS_API_KEY and rlaif_trainer:\n",
    "    print(\"\\nRLAIF оценка и обучение...\")\n",
    "    \n",
    "    result = rlaif_trainer.evaluate_and_train(\n",
    "        context=context,\n",
    "        task_description=task_def.description,\n",
    "        result=context.result\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nОценка: {result.average_score:.2f}\")\n",
    "    print(f\"Инструментов обучено: {result.tools_updated}\")\n",
    "    print(\"\\nИзменения репутации:\")\n",
    "    for tool_name, delta in result.improvements.items():\n",
    "        print(f\"  {tool_name}: {delta:+.4f}\")\n",
    "else:\n",
    "    print(\"\\nСимуляция: Оценка 0.87, репутация +0.0174\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Итоги\n",
    "\n",
    "**RLAIF протестирован**:\n",
    "- ✓ LLM критик оценивает по 4 критериям\n",
    "- ✓ Автоматическое обучение\n",
    "- ✓ Обновление репутации\n",
    "- ✓ Без участия человека\n",
    "\n",
    "**Преимущества**:\n",
    "- Масштабируемость (тысячи оценок)\n",
    "- Скорость (мгновенная обратная связь)\n",
    "- Стоимость ($0.001-0.01 за оценку)\n",
    "- Consistency (одинаковые критерии)\n",
    "\n",
    "**Применение**:\n",
    "- Автоматическая валидация\n",
    "- Continuous обучение\n",
    "- A/B тестирование инструментов"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
